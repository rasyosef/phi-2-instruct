{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLvLIyZaJPtm"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi\n",
        "! rm -r phi-2-sft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U --quiet datasets evaluate torch transformers accelerate trl peft"
      ],
      "metadata": {
        "id": "SFMfwUzJJqdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "6K51GTgZanMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**"
      ],
      "metadata": {
        "id": "PFp18aebJwuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "OpenHermes = load_dataset(\"rasyosef/OpenHermes-SLM-384k\", split=\"train\")\n",
        "OpenHermes"
      ],
      "metadata": {
        "id": "ANOvGeOyJzwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\",\n",
        "    #attn_implementation=\"flash_attention_2\",\n",
        "  )"
      ],
      "metadata": {
        "id": "OyeeHfQLJ20b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import setup_chat_format\n",
        "\n",
        "# Set up the chat format with default 'chatml' format\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "metadata": {
        "id": "MqxcT5xYJ6M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"Hello, there!\"}, {\"role\": \"assistant\", \"content\": \"Hi!\"}], tokenize=False)"
      ],
      "metadata": {
        "id": "6KfE3iNbJ8Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "OxigSlxtJ-G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Filter Dataset**"
      ],
      "metadata": {
        "id": "SDcwgi9NKKLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 448\n",
        "OpenHermes_filtered = OpenHermes.filter(lambda row: row[\"phi_token_count\"] < MAX_LENGTH)\n",
        "OpenHermes_filtered"
      ],
      "metadata": {
        "id": "mFNgD8OkKJFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "num_messages = OpenHermes_filtered[\"length\"]\n",
        "print(dict(Counter(num_messages)))"
      ],
      "metadata": {
        "id": "Oj9fFlLZKR19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 128_000\n",
        "OpenHermes_Final = OpenHermes_filtered.shuffle(seed=42).select(range(NUM_SAMPLES))\n",
        "OpenHermes_Final = OpenHermes_Final.train_test_split(test_size=0.01, seed=42)\n",
        "OpenHermes_Final"
      ],
      "metadata": {
        "id": "oJkfwnqSKUAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_messages = OpenHermes_Final[\"train\"][\"length\"]\n",
        "Counter(num_messages)"
      ],
      "metadata": {
        "id": "3a7IxgSeKX8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(OpenHermes_Final[\"train\"][\"messages_templated\"][i])\n",
        "  print(\"\\n-------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "xm5v9uSlKatQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LoRA Adapter**"
      ],
      "metadata": {
        "id": "t9agUSB5Kbxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, cast_mixed_precision_params\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Target all linear layers\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\", \"lm_head\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "cast_mixed_precision_params(model, dtype=torch.float16)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "23f1ArTpKfG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TO CONTINUE TRAINING\n",
        "\n",
        "# from peft import PeftModel, cast_mixed_precision_params\n",
        "\n",
        "# peft_model_id = \"rasyosef/phi-2-sft-openhermes-128k-v2\"\n",
        "# model = PeftModel.from_pretrained(model, peft_model_id, is_trainable=True)\n",
        "# cast_mixed_precision_params(model, dtype=torch.float16)\n",
        "# model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "_SOHFgbojz52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SFT with TRL**"
      ],
      "metadata": {
        "id": "G1jXqvjvKiCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "max_seq_length = 768\n",
        "\n",
        "batch_size = 4 # On a T4 or P100 GPU, batch_size should be set to 1 to avoid cuda out of memroy error\n",
        "gradient_accum_steps = 4\n",
        "epochs = 2\n",
        "\n",
        "new_model_id = \"phi-2-sft\"\n",
        "\n",
        "eval_steps = 200\n",
        "save_steps = eval_steps * 2\n",
        "logging_steps=eval_steps\n",
        "\n",
        "print(\"Eval Steps:\", eval_steps)\n",
        "print(\"Save Steps:\", save_steps)\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    dataset_text_field=\"messages_templated\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    output_dir=new_model_id,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accum_steps,\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=4e-5,\n",
        "    warmup_steps=400,\n",
        "    lr_scheduler_type=\"linear\", # could also use a cosine scheduler\n",
        "    fp16=True,\n",
        "    packing=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=logging_steps,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=eval_steps,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit = 1,\n",
        "    neftune_noise_alpha=5, # NEFTune\n",
        "    seed=42,\n",
        "    # push_to_hub=True, # Uncomment this line to push model to huggingface hub\n",
        "    # hub_token=userdata.get(\"HF_TOKEN\"), # uncomment this line to set your huggingface hub write token. This must be set if push_to_hub=True\n",
        "\n",
        "    # gradient_checkpointing=True,\n",
        "    # gradient_checkpointing_kwargs={'use_reentrant':False}\n",
        "  )"
      ],
      "metadata": {
        "id": "qV1nWy06Kkxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    args=sft_config,\n",
        "    train_dataset=OpenHermes_Final['train'],\n",
        "    eval_dataset=OpenHermes_Final['test'],\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "DBZnlPDDKtWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "kaoEQJNqK1-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "dJteW8X5K4SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "FW-VNrPVK59m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(messages):\n",
        "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(tokenized_chat, max_new_tokens=128) #, stopping_criteria=[\"<|im_end|>\"])\n",
        "    print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Who is Leonhard Euler?\"}]\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "qUoxrUXMK7xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"What is quantum computing?\"}]\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "W6t8FbB0K-Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"Do you have any jokes about hats?\"}]\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "fJF2Nh4QK_wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "MmFptuDhLAnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HRHtJ27l5FVE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}